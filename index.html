<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Real-Time Tracking of Dynamic Objects Using 3D Gaussian Splatting</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 980px;
      margin: 0 auto;
      padding: 20px;
      line-height: 1.6;
    }
    h1, h2, h3 {
      color: #0a2f44;
    }
    img {
      max-width: 100%;
      margin: 1rem 0;
    }
    .caption {
      font-size: 0.9em;
      text-align: center;
      color: #666;
    }
    code {
      background: #f4f4f4;
      padding: 2px 4px;
    }
  </style>
</head>
<body>
  <h1>3D Gaussian Splatting for Real-Time Tracking of Dynamic Objects Using Monocular View</h1>
  <p><strong>Authors:</strong> Saurabh Vidyarthi</p>
  <p><strong>Institution:</strong> Indian Institute of Science, Bangalore</p>

  <h2>Abstract</h2>
  <p>This work introduces a training-free method that updates the pose of moving objects in a static 3D Gaussian Splatting (3DGS) scene using only monocular RGB input. It preserves appearance fidelity by only modifying Gaussian positions and orientations, enabling fast and drift-free dynamic scene reconstruction suitable for real-time robotics and digital twin applications.</p>

  <h2>1. Introduction</h2>
  <p>3D Gaussian Splatting (3DGS) offers real-time, high-fidelity 3D scene rendering by representing environments as millions of 3D Gaussians. However, its dynamic extensions require expensive retraining for each update. Our proposed pipeline overcomes this bottleneck by enabling selective, real-time pose updates for dynamic Gaussians, using a monocular camera and semantic segmentation.</p>

  <h2>2. Related Work</h2>
  <p>We build upon recent advances in:</p>
  <ul>
    <li><strong>3D Gaussian Splatting</strong> for photorealistic rendering of static scenes</li>
    <li><strong>Gradient Backprojection</strong> for infusing semantic features into Gaussians</li>
  </ul>

  <h2>3. Method I: Uplifting Pointmap Transform</h2>
  <h3>3.1 Semantic Scene Initialization</h3>
  <p>A multi-view image set is used to construct the static 3DGS scene. Gradient-Weighted Backprojection embeds semantic features into Gaussians. Dynamic elements are masked out based on class labels (e.g., person, robot).</p>

  <h3>3.2 Image-Based Object Tracking</h3>
  <ul>
    <li>Rendered and live monocular images are fed into YOLO for object detection</li>
    <li>MoGe generates dense point maps for detected objects in both views</li>
    <li>Point map based rigid body transform is estimated for each object</li>
  </ul>

  <h3>3.3 Pose-Only Update</h3>
  <p>Estimated transform is scaled to match the 3DGS metric space and applied to dynamic Gaussians. Only position and orientation are updated, leaving appearance unchanged.</p>

  <img src="main_poinmap_flow.jpg" alt="Pipeline Flow Diagram">
  <p class="caption">Figure: Overall pipeline for monocular 3DGS update using pointmaps.</p>


  <!-- <h2>4. Scale Recovery and Evaluation</h2>
  <p>We investigate both isotropic and anisotropic scale calibration methods. Empirical results show that a neural-network-based anisotropic scale predictor improves alignment accuracy.</p>

  <img src="graph.jpg" alt="Accuracy Graph">
  <p class="caption">Figure: Normalized Translation Error vs Time shows stability under Method II.</p> -->

  <h2>5. Results and Visualizations</h2>
  <img src="ex_01.png">
  <img src="ex_02.jpg">
  <img src="ex_03.jpg">
  <div style="display: flex; gap: 20px;">
  <video width="48%" controls autoplay loop muted>
    <source src="intial_farad.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <video width="48%" controls autoplay loop muted>
    <source src="fard_final.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
</div>
<!-- <p class="caption">Figure: Real-time tracking â€“ initial vs. updated object pose in 3DGS.</p> -->

  <!-- <h2>7. Qualitative Evaluation</h2> -->
  <img src="ex_03.drawio.png">
  <p class="caption">Figure: Multi-object tracking under indoor and outdoor conditions</p>

  <h2>8. Conclusion</h2>
  <p>This work demonstrates that real-time, training-free tracking of dynamic objects in a semantically enriched 3D Gaussian field is feasible with monocular RGB input. Our method preserves scene photorealism and semantic consistency, delivering low-latency performance suitable for real-world robotics and digital twin applications.</p>

</body>
</html>
